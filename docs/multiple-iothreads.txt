Copyright (c) 2014 Red Hat Inc.

This work is licensed under the terms of the GNU GPL, version 2 or later.  See
the COPYING file in the top-level directory.


This document explains the IOThread feature and how to write code that runs
outside the QEMU global mutex.

The main loop and IOThreads
---------------------------
QEMU is an event-driven program that can do several things at once using an
event loop.  The VNC server and the QMP monitor are both processed from the
same event loop, which monitors their file descriptors until they become
readable and then invokes a callback.

The default event loop is called the main loop (see main-loop.c).  It is
possible to create additional event loop threads using -object
iothread,id=my-iothread.

Side note: The main loop and IOThread are both event loops but their code is
not shared completely.  Sometimes it is useful to remember that although they
are conceptually similar they are currently not interchangeable.

Why IOThreads are useful
------------------------
IOThreads allow the user to control the placement of work.  The main loop is a
scalability bottleneck on hosts with many CPUs.  Work can be spread across
several IOThreads instead of just one main loop.  When set up correctly this
can improve I/O latency and reduce jitter seen by the guest.

The main loop is also deeply associated with the QEMU global mutex, which is a
scalability bottleneck in itself.  vCPU threads and the main loop use the QEMU
global mutex to serialize execution of QEMU code.  This mutex is necessary
because a lot of QEMU's code historically was not thread-safe.

The fact that all I/O processing is done in a single main loop and that the
QEMU global mutex is contended by all vCPU threads and the main loop explain
why it is desirable to place work into IOThreads.

The experimental virtio-blk data-plane implementation has been benchmarked and
shows these effects:
ftp://public.dhe.ibm.com/linux/pdfs/KVM_Virtualized_IO_Performance_Paper.pdf

How to program for IOThreads
----------------------------
The main difference between legacy code and new code that can run in an
IOThread is dealing explicitly with the event loop object, AioContext
(see include/block/aio.h).  Code that only works in the main loop
implicitly uses the main loop's AioContext, and does not need to handle
locking because it implicitly uses the QEMU global mutex.

Code that supports running in IOThreads, instead, must be aware of its
AioContext and of synchronization.

AioContext supports the following services:
 * File descriptor monitoring (read/write/error on POSIX hosts)
 * Event notifiers (inter-thread signalling)
 * Timers
 * Bottom Halves (BH) deferred callbacks

There are several old APIs that use the main loop AioContext:
 * LEGACY qemu_aio_set_fd_handler() - monitor a file descriptor
 * LEGACY qemu_aio_set_event_notifier() - monitor an event notifier
 * LEGACY timer_new_ms() - create a timer
 * LEGACY qemu_bh_new() - create a BH
 * LEGACY qemu_aio_wait() - run an event loop iteration

Since they implicitly work on the main loop they cannot be used in code that
runs in an IOThread.  They might cause a crash or deadlock if called from an
IOThread since the QEMU global mutex is not held.

Instead, use the AioContext functions directly (see include/block/aio.h):
 * aio_set_fd_handler() - monitor a file descriptor
 * aio_set_event_notifier() - monitor an event notifier
 * aio_timer_new() - create a timer
 * aio_bh_new() - create a BH
 * aio_poll() - run an event loop iteration

The AioContext can be obtained from the IOThread using
iothread_get_aio_context() or for the main loop using qemu_get_aio_context().
Code that takes an AioContext argument works both in IOThreads or the main
loop, depending on which AioContext instance the caller passes in.

When running in an IOThread, handlers for bottom halves, file descriptors,
and timers have to handle their own synchronization.  Block layer AIO
callbacks also have to handle synchronization.  How to do this is the
topic of the next sections.

How to synchronize with an IOThread
-----------------------------------
AioContext is not thread-safe so some rules must be followed when using file
descriptors, event notifiers, timers, or BHs across threads:

1. AioContext functions can always be called safely.  They handle their
own locking internally.

2. AioContext does not attempt to provide mutual exclusion of any kind
to bottom half handlers, file descriptor handlers, timer handlers and AIO
callbacks.  aio_context_acquire() and aio_context_release() are the
typical choice; aio_context_acquire()/aio_context_release() calls may
be nested.

3. Other threads wishing to access the AioContext must take the QEMU global
mutex *as well as* call aio_context_acquire()/aio_context_release() for
mutual exclusion.  The QEMU global mutex must be taken outside the call
to aio_context_acquire().

No lock ordering rule is necessary if a thread needs to acquire multiple
AioContexts simultaneously.  Because the IOThreads won't ever acquire
multiple AioContexts, it is always safe for the owner of the QEMU global
mutex to acquire any number of them.

Side note: the best way to schedule a function call across threads is to create
a BH in the target AioContext beforehand and then call qemu_bh_schedule().  No
acquire/release or locking is needed for the qemu_bh_schedule() call.  But be
sure to acquire the AioContext for aio_bh_new() if necessary.

AioContext and the block layer
------------------------------
The AioContext originates from the QEMU block layer, even though nowadays
AioContext is a generic event loop that can be used by any QEMU subsystem.

The block layer has support for AioContext integrated.  Each BlockDriverState
is associated with an AioContext using bdrv_set_aio_context() and
bdrv_get_aio_context().  This allows block layer code to process I/O inside the
right AioContext.  Other subsystems may wish to follow a similar approach.

Block layer code must therefore expect to run in an IOThread and avoid using
old APIs that implicitly use the main loop.  See the "How to program for
IOThreads" above for information on how to do that.

If main loop code such as a QMP function wishes to access a BlockDriverState
it must first call aio_context_acquire(bdrv_get_aio_context(bs)) to ensure
that callbacks in the IOThread do not run in parallel.

Code running in the monitor typically needs to ensure that past
requests from the guest are completed.  When a block device is running
in an IOThread, the IOThread can also process requests from the guest
(via ioeventfd).  To achieve both objects, wrap the code between
bdrv_drained_begin() and bdrv_drained_end(), thus creating a "drained
section".  The functions must be called between aio_context_acquire()
and aio_context_release().  You can freely release and re-acquire the
AioContext within a drained section.

AioContext and coroutines
-------------------------
Long-running jobs (usually in the form of coroutines) are best scheduled in
the BlockDriverState's AioContext to avoid the need to acquire/release around
each bdrv_*() call.  The functions bdrv_add/remove_aio_context_notifier,
or alternatively blk_add/remove_aio_context_notifier if you use BlockBackends,
can be used to get a notification whenever bdrv_set_aio_context() moves a
BlockDriverState to a different AioContext.

Usually, the coroutine is created and first entered in the main loop.  After
some time it will yield and, when entered again, it will run in the
IOThread.  In general, however, the coroutine never has to worry about
releasing and acquiring the AioContext.  The release will happen in the
function that entered the coroutine; the acquire will happen (e.g. in
a bottom half or AIO callback) before the coroutine is entered.  For
example:

1) the coroutine yields because it waits on I/O (e.g. during a call to
bdrv_co_readv) or sleeps for a determinate period of time (e.g. with
co_aio_sleep_ns).  In this case, functions such as bdrv_co_io_em_complete
or co_sleep_cb take the AioContext lock before re-entering.

2) the coroutine explicitly yields.  In this case, the code implementing
the long-running job will have a bottom half, AIO callback or similar that
should acquire the AioContext before re-entering the coroutine.

3) the coroutine yields through a CoQueue (directly or indirectly, e.g.
through a CoMutex or CoRwLock).  This also "just works", but the mechanism
is more subtle.  Suppose coroutine C1 has just released a mutex and C2
was waiting on it.  C2 is woken up at C1's next yield point, or just
before C1 terminates; C2 then runs *before qemu_coroutine_enter(C1)
returns to its caller*.  Assuming that C1 was entered with a sequence like

    aio_context_acquire(ctx);
    qemu_coroutine_enter(co);     // enters C1
    aio_context_release(ctx);

... then C2 will also be protected by the AioContext lock.


Bullet 3 provides the following rule for using coroutine mutual exclusion
in multiple threads:

    *** If two coroutines can exclude each other through a CoMutex or
    *** CoRwLock or CoQueue, they should all run under the same AioContext.
